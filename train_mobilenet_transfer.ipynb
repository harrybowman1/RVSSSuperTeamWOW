{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see:\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py and \n",
    "https://pytorch.org/hub/pytorch_vision_mobilenet_v2/\n",
    "\n",
    "Datasets\n",
    "\n",
    "- data1... shitty run. The test set is a random subset\n",
    "- data2... less shitty run. Test-Data is the last frames\n",
    "- data5... good run with a little noise (wheel outside road)\n",
    "- data5_subset... subset of data5 to be used as a evaluation dataset\n",
    "- data6... perfrect run on the green data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problem**: Acc konvergiert nicht\n",
    "\n",
    "\n",
    "# Mögliche Ursachen\n",
    "\n",
    "* Model\n",
    "    * kann icht sein, denn es funktioniert auch nicht mit Harry's Model, und er sagt, er bekommt mehr\n",
    "    * ich habe auch schon probiert, ein komplexeres FC-Modell zu nutzen, und das hat auch nicht geklappt\n",
    "* Training\n",
    "    * kann icht sein, denn der Train-Loop ist straight forward\n",
    "* Daten\n",
    "    * trainset und testset sind zu unterschiedlich\n",
    "* Berechnung der Acc.\n",
    "* auf den Daten ist gar keine Generalisierungsfähigkeit zu erwarten\n",
    "\n",
    "# Lösungen\n",
    "\n",
    "## Lösung 1\n",
    "* möglichst geradeaus fahren und kurz vor der Kurve drehen\n",
    "* data6 ist der beste DAtensatz bis jetzt\n",
    "* data5 ist auch nicht schlecht und geht in die Gegenrichtung von data6, die beiden Datensätze muss ich im folgenden benutzen\n",
    "=> mit dem neuen Datensatz und dem Modell von Harry erziele ich bessere Ergebnisse\n",
    "\n",
    "## Lösung 2\n",
    "* der Transfer funktioniert schlecht, da ich an einer zu späten stelle cutte\n",
    "* früher cutten\n",
    "\n",
    "TODO\n",
    "- [x] mein Modell langsam wieder einführen\n",
    "- [ ] alles auf dem Roboter testen\n",
    "- [ ] einen neuen Datensatz auf einer Schotterstraßenkombination testen\n",
    "- [ ] Hyperparameter weiter tweaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "TRAIN_DIR = 'data6'\n",
    "TEST_DIR = 'data5_subset'\n",
    "DOWNSIZE = (32, 32)\n",
    "HEIGHT = 240\n",
    "WIDTH = 320\n",
    "CROP_FRAC = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.nn.modules.flatten.Flatten is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m     13\u001b[0m resnet \u001b[39m=\u001b[39m resnet18(weights\u001b[39m=\u001b[39mResNet18_Weights\u001b[39m.\u001b[39mDEFAULT)\n\u001b[0;32m---> 14\u001b[0m embedding_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mSequential(\u001b[39m*\u001b[39;49m\u001b[39mlist\u001b[39;49m(resnet\u001b[39m.\u001b[39;49mchildren())[:\u001b[39m5\u001b[39;49m], torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mFlatten)\n\u001b[1;32m     16\u001b[0m \u001b[39m#mobilenet = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#embedding_model = torch.nn.Sequential(*list(list(mobilenet.children())[0].children())[:2])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m embedding_model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.mambaforge/envs/rvss/lib/python3.10/site-packages/torch/nn/modules/container.py:91\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args):\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(idx), module)\n",
      "File \u001b[0;32m~/.mambaforge/envs/rvss/lib/python3.10/site-packages/torch/nn/modules/module.py:444\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[39mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(module, Module) \u001b[39mand\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not a Module subclass\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    445\u001b[0m         torch\u001b[39m.\u001b[39mtypename(module)))\n\u001b[1;32m    446\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, torch\u001b[39m.\u001b[39m_six\u001b[39m.\u001b[39mstring_classes):\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule name should be a string. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    448\u001b[0m         torch\u001b[39m.\u001b[39mtypename(name)))\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.nn.modules.flatten.Flatten is not a Module subclass"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights, resnet18, ResNet18_Weights\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:5], torch.nn.Flatten)\n",
    "\n",
    "#mobilenet = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "#embedding_model = torch.nn.Sequential(*list(list(mobilenet.children())[0].children())[:2])\n",
    "embedding_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1024, out_features=256, bias=True),\n",
    "    #torch.nn.Linear(in_features=384, out_features=256, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=128, out_features=3, bias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,root_folder,img_ext = \".jpg\" , transform=None):\n",
    "        self.root_folder = root_folder\n",
    "        self.transform = transform        \n",
    "        self.img_ext = img_ext        \n",
    "        self.filenames = glob(path.join(self.root_folder,\"*\" + self.img_ext))            \n",
    "        self.totensor = transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        f = self.filenames[idx]        \n",
    "        img = Image.open(f)\n",
    "        \n",
    "        if self.transform == None:\n",
    "            img = self.totensor(img)\n",
    "        else:\n",
    "            img = self.transform(img)   \n",
    "        \n",
    "        if 'Ang' in f:\n",
    "            steering = f.split('Ang')[1][:-4]\n",
    "            steering = float(steering)\n",
    "        else:\n",
    "            steering = f.split(\"/\")[-1].split(self.img_ext)[0][6:]\n",
    "            if '-' in steering:\n",
    "                steering = steering.split('.')[-1]\n",
    "            steering = np.float32(steering)\n",
    "\n",
    "        if steering > 0:\n",
    "            simple_steering = 0\n",
    "        elif steering < 0:\n",
    "            simple_steering = 1\n",
    "        else:\n",
    "            simple_steering = 2\n",
    "    \n",
    "        sample = {\"image\":img , \"steering\":steering, \"simple_steering\": simple_steering}        \n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: transforms.functional.crop(x, int(HEIGHT/CROP_FRAC), 0, int(HEIGHT-HEIGHT/CROP_FRAC), WIDTH)),\n",
    "    transforms.Resize(DOWNSIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = SteerDataSet(TRAIN_DIR, \".jpg\", preprocess)\n",
    "testset = SteerDataSet(TEST_DIR, \".jpg\", preprocess)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(preds, label, bs):\n",
    "    return sum( [a.item() == b.item()  for (a, b) in zip(preds, label)] ) / bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_FREQ = 20\n",
    "\n",
    "classifier.train()\n",
    "test_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 20):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, s in enumerate(trainloader):\n",
    "        # bring data in right format\n",
    "        data = s['image']\n",
    "        label = s['simple_steering']\n",
    "        # label_onehot = torch.nn.functional.one_hot(label, num_classes = 3)\n",
    "\n",
    "        # apply embedding model\n",
    "        with torch.no_grad():\n",
    "            embeddings = torch.flatten(embedding_model(data), 1)\n",
    "        \n",
    "        # backprop classifier\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(embeddings)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update train-accuracy\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        this_acc = get_acc(pred, label, BATCH_SIZE)\n",
    "\n",
    "        running_loss = running_loss + loss.item()\n",
    "        running_acc = running_acc + this_acc\n",
    "\n",
    "        if i % PRINT_FREQ == PRINT_FREQ-1:\n",
    "            print(f\"epoch: {epoch},\\t item: {i+1},\\t loss:{running_loss:.3f},\\t acc: {running_acc/PRINT_FREQ:.2f}\")\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "    # TEST LOOP\n",
    "    print('test loop')\n",
    "    running_test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, s in enumerate(testloader):\n",
    "            data = s['image']\n",
    "            label = s['simple_steering']\n",
    "\n",
    "            embeddings = torch.flatten(embedding_model(data), 1)       \n",
    "            output = classifier(embeddings)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            test_acc = get_acc(pred, label, BATCH_SIZE)\n",
    "            running_test_acc = test_acc + running_test_acc\n",
    "\n",
    "        print(f\"test acc: {running_test_acc / len(testloader):.2f}\")\n",
    "        test_accuracies.append(running_test_acc / len(testloader))\n",
    "\n",
    "    torch.save(classifier.state_dict(), f'class_params_e{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(test_accuracies)\n",
    "plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1669dcdbca07850241162f699789cbb9152ec44917846b1cbad02e6b684a829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
